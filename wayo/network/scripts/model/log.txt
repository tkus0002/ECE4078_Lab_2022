Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.001
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: /home/jumanji1310/catkin_ws/src/data_collector/data_collector/dataset
===========================================================
============= Epoch 0 | 2022-09-05 16:35:03 ===============
=> Current Lr: 0.001
[0/62]: 1.8821
[20/62]: 0.0558
[40/62]: 0.0415
[60/62]: 0.0310
=> Training Loss: 0.0958, Evaluation Loss 0.0400

============= Epoch 1 | 2022-09-05 16:45:13 ===============
=> Current Lr: 0.001
[0/62]: 0.0407
============= Epoch 1 | 2022-09-05 17:18:11 ===============
=> Current Lr: 0.001
[0/62]: 0.0390
[20/62]: 0.0344
[40/62]: 0.0228
[60/62]: 0.0300
=> Training Loss: 0.0330, Evaluation Loss 0.0298

============= Epoch 2 | 2022-09-05 17:25:43 ===============
=> Current Lr: 0.001
[0/62]: 0.0251
[20/62]: 0.0287
[40/62]: 0.0270
[60/62]: 0.0226
============= Epoch 2 | 2022-09-05 18:45:03 ===============
=> Current Lr: 0.001
[0/62]: 0.0370
[20/62]: 0.0253
[40/62]: 0.0207
[60/62]: 0.0194
=> Training Loss: 0.0249, Evaluation Loss 0.0210

============= Epoch 3 | 2022-09-05 18:52:11 ===============
=> Current Lr: 0.001
[0/62]: 0.0183
[20/62]: 0.0185
[40/62]: 0.0169
[60/62]: 0.0185
=> Training Loss: 0.0197, Evaluation Loss 0.0197

============= Epoch 4 | 2022-09-05 18:59:17 ===============
=> Current Lr: 0.001
[0/62]: 0.0149
[20/62]: 0.0171
[40/62]: 0.0238
[60/62]: 0.0186
=> Training Loss: 0.0189, Evaluation Loss 0.0161

============= Epoch 5 | 2022-09-05 19:06:21 ===============
=> Current Lr: 0.0005
[0/62]: 0.0136
[20/62]: 0.0132
[40/62]: 0.0136
[60/62]: 0.0168
=> Training Loss: 0.0141, Evaluation Loss 0.0139

============= Epoch 6 | 2022-09-05 19:13:30 ===============
=> Current Lr: 0.0005
[0/62]: 0.0113
[20/62]: 0.0094
[40/62]: 0.0114
[60/62]: 0.0163
=> Training Loss: 0.0121, Evaluation Loss 0.0136

============= Epoch 7 | 2022-09-05 19:20:37 ===============
=> Current Lr: 0.0005
[0/62]: 0.0171
[20/62]: 0.0114
[40/62]: 0.0117
[60/62]: 0.0125
=> Training Loss: 0.0124, Evaluation Loss 0.0135

============= Epoch 8 | 2022-09-05 19:27:41 ===============
=> Current Lr: 0.0005
[0/62]: 0.0107
[20/62]: 0.0097
[40/62]: 0.0105
[60/62]: 0.0113
=> Training Loss: 0.0117, Evaluation Loss 0.0110

============= Epoch 9 | 2022-09-05 19:34:45 ===============
=> Current Lr: 0.0005
[0/62]: 0.0101
[20/62]: 0.0078
[40/62]: 0.0080
[60/62]: 0.0082
=> Training Loss: 0.0093, Evaluation Loss 0.0093

============= Epoch 10 | 2022-09-05 19:41:48 ==============
=> Current Lr: 0.00025
[0/62]: 0.0090
[20/62]: 0.0069
[40/62]: 0.0063
[60/62]: 0.0072
=> Training Loss: 0.0082, Evaluation Loss 0.0088

============= Epoch 11 | 2022-09-05 19:48:52 ==============
=> Current Lr: 0.00025
[0/62]: 0.0094
[20/62]: 0.0080
[40/62]: 0.0070
[60/62]: 0.0091
=> Training Loss: 0.0076, Evaluation Loss 0.0080

============= Epoch 12 | 2022-09-05 19:55:56 ==============
=> Current Lr: 0.00025
[0/62]: 0.0063
[20/62]: 0.0078
[40/62]: 0.0079
[60/62]: 0.0077
=> Training Loss: 0.0076, Evaluation Loss 0.0082

============= Epoch 13 | 2022-09-05 20:03:01 ==============
=> Current Lr: 0.00025
[0/62]: 0.0080
[20/62]: 0.0117
[40/62]: 0.0064
[60/62]: 0.0061
=> Training Loss: 0.0076, Evaluation Loss 0.0071

============= Epoch 14 | 2022-09-05 20:10:07 ==============
=> Current Lr: 0.00025
[0/62]: 0.0067
[20/62]: 0.0053
[40/62]: 0.0118
[60/62]: 0.0056
=> Training Loss: 0.0069, Evaluation Loss 0.0078

============= Epoch 15 | 2022-09-05 20:17:14 ==============
=> Current Lr: 0.000125
[0/62]: 0.0108
[20/62]: 0.0064
[40/62]: 0.0062
[60/62]: 0.0063
=> Training Loss: 0.0063, Evaluation Loss 0.0064

============= Epoch 16 | 2022-09-05 20:24:25 ==============
=> Current Lr: 0.000125
[0/62]: 0.0047
[20/62]: 0.0049
[40/62]: 0.0052
[60/62]: 0.0086
=> Training Loss: 0.0061, Evaluation Loss 0.0060

============= Epoch 17 | 2022-09-05 20:31:33 ==============
=> Current Lr: 0.000125
[0/62]: 0.0049
[20/62]: 0.0055
[40/62]: 0.0067
[60/62]: 0.0054
=> Training Loss: 0.0056, Evaluation Loss 0.0059

============= Epoch 18 | 2022-09-05 20:38:40 ==============
=> Current Lr: 0.000125
[0/62]: 0.0053
[20/62]: 0.0062