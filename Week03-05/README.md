# Milestone 2: Simultaneous Localization And Mapping (SLAM)
- [Introduction](#Introduction)
    - [ARUCO markers](#ARUCO-markers)
    - [SLAM helper scripts](#SLAM-helper-scripts)
- [Activities](#Activities)
    - [Calibration (week 3)](#Calibration-week-3)
    - [SLAM (week 4-5)](#SLAM-week-4-5)
- [M2 Marking](#M2-Marking)
    - [Evaulation scheme](#evaluation-scheme)
    - [Instructions](#marking-instructions)
- [Technical FAQs](#FAQs-M2)


## Introduction
### ARUCO markers
[ARUCO markers](http://www.uco.es/investiga/grupos/ava/node/26) are square fiducial markers introduced by Rafael Mu√±oz and Sergio Garrido. OpenCV contains a trained [function](https://docs.opencv.org/trunk/d5/dae/tutorial_aruco_detection.html) that detects the ARUCO markers, which will be used in this project (the dictionary we used to generate the markers was ```cv2.aruco.DICT_4X4_100```). PenguinPi will be using these ARUCO markers as road-signs to help it map the environment and locate itself. 

You'll need to install required packages by typing the following commands in the terminal:
```
python3 -m pip install machinevision-toolbox-python spatialmath-python==0.8.9 opencv-contrib-python==4.1.2.30 matplotlib
cd ~/catkin_ws/
source ~/catkin_ws/devel/setup.bash
catkin_make
```

To confirm the required packages are installed, in terminal type ```python3```, which opens python 3 in command line, then type:
```
import cv2
from cv2 import aruco
from machinevisiontoolbox import Image, CentralCamera
```
If there is no error then everything is successfully installed and you can exit the command line python 3 by typing ```exit()```


### SLAM helper scripts
The following helper scripts are provided to support your development of SLAM
- [operate.py](operate.py) is the central control program that combines both keyboard teleoperation (implemented as mart of Milestone 1 (M1) and SLAM. Running this file requires the [utility scripts](../Week01-02/util) and the [GUI pics](../Week01-02/pics) of M1, and the [calibration parameters](calibration/param/) and [SLAM scripts](slam/) of M2. After the GUI is launched, you can start or stop SLAM by presessing ENTER, and save a map generated by SLAM by pressing "s". It provides a visualization of the robot's view with overlay of identified ARUCO markers, the SLAM visualization, and a 5 minute count-down clock. Note: remember to replace the [keyboard control section](operate.py#L194) with your codes from M1.
- [A simulator world](gazebo_calib/) for wheel and camera calibration, together with the [calibration scripts](calibration/) and [default parameters](calibration/param/): see [calibration section](#Calibration-week-3) for details
- [aruco_detector.py](slam/aruco_detector.py) uses OpenCV to detect ARUCO markers and provides an estimation to their positions, which will be used for SLAM in addition to the drive signals
- [mapping_utils.py](slam/mapping_utils.py) saves the SLAM map for evaluation
- [SLAM_eval.py](SLAM_eval.py) evaluates a SLAM map against the ground truth


## Activities
### Calibration (week 3)

#### Step 1) Preparing Working directory
Prior to working on this week's materials, please make sure you do the following:
- Copy over the [util folder](../Week01-02/util) and the [GUI pics](../Week01-02/pics) from the M1 lab into your working directory
- Replace the [keyboard control section](operate.py#L194) in operate.py with the code you developed for M1

#### Step 2a) Wheel Calibration (Simulation)
**Please complete [wheel_calibration.py](calibration/wheel_calibration.py) by filling in [line 46](calibration/wheel_calibration.py#L46) for computing the scale parameter, and [line 89](calibration/wheel_calibration.py#L89) for computing the baseline parameter.** 

For wheel calibration inside the simulator, first launch the world by typing the following commands in your terminal (you don't need to generate any objects in the world for wheel calibration):
```
source ~/catkin_ws/devel/setup.bash
roslaunch penguinpi_gazebo ECE4078.launch
```

Then in another terminal, run the [wheel calibration script](calibration/wheel_calibration.py) using the command ```python3 wheel_calibration.py```. This script will set the robot driving forward, and then spinning at various velocities, for durations that you specify, in order to compute the scale and baseline parameters for wheel calibration.

On the Gazebo ground plane, each square grid is 1m^2. You can use that to measure if the robot has moved 1m, or rotated for 360deg. The script will prompt you to specify how long the robot should drive, or spin, and then confirm whether it has travelled or rotated the required amount.
It may be helpful to reset the robot's pose after each run. To do this, click on the robot, or select it from the list of models in the left panel. Expand the "pose" section in the left panel. To reset the robot's pose, and manually reset the values in 'x', 'y', and 'yaw' to 0. (Note: after you change its pose, the robot may not immediately show in Gazebo). Avoid using the translation (on z axis) /rotation mode on the robot as this will destroy its balance and take a long time to recover. 
If, after the robot has moved, the values still appear to be 0, select the roboot from the list of models again (take care to ensure you haven't selected any of the links instead), and the values should update.

Try to be as accurate as possible when determining how far the robot has travelled, or spun, in order to improve the accuracy of your calibration calculations.

![Changing robot's pose in Gazebo](screenshots/GazeboPose.png?raw=true "Changing robot's pose in Gazebo")


#### Step 2b) Wheel calibration (Physical robot)
Similar to the previous step, once you have completed [wheel_calibration.py](calibration/wheel_calibration.py) by filling in the required lines of code, run the [wheel calibration script](calibration/wheel_calibration.py) using the command ```python3 wheel_calibration.py --ip 192.168.50.1 --port 8080```. This script will set the robot driving forward, and then spinning at various velocities, for durations that you specify, in order to compute the scale and baseline parameters for wheel calibration.

You can mark a 1m long straight line with masking tape on the floor, and use it as a guide to check if the robot has travelled exactly (as close as possible) 1m. Masking tape and measuring tape will be provided to you in the lab. 


#### Step 3a) Camera calibration (Simulation)
To set up the calibration world in the simulator, copy the [rig folder](gazebo_calib/rig) to your catkin_ws/src/pernguinpi_gazebo/models/, [calibration.launch](gazebo_calib/calibration.launch) to your catkin_ws/src/pernguinpi_gazebo/launch/, and [calibration.world](gazebo_calib/calibration.world) to your catkin_ws/src/pernguinpi_gazebo/worlds/

You can then launch the camera calibration world using the commands below (first time launching the calibration world may take a little while):
```
source ~/catkin_ws/devel/setup.bash
roslaunch penguinpi_gazebo calibration.launch
```

![The calibration world in Gazebo](screenshots/GazeboCameraCalib.png?raw=true "The calibration world in Gazebo")

Take a photo of the caliabration rig using [calib_pic.py](calibration/calib_pic.py) (located in the calibration folder). Run the script using the command ```python3 calib_pic.py```, and take a calibration photo by pressing ENTER while running the script.

You may also drive the robot around with arrow keys to a preferred location where all 8 calibration points are clearly visible before taking the photo. You will need to replace the [keyboard control section](calibration/calib_pic.py#L31) of this file with your codes from M1 to enable the driving functions. The calibration photo is saved as [calib_0.png](calibration/calib_0.png).

You will now need to calibrate the camera parameters, using using [camera_calibration.py](calibration/camera_calibration.py). This will require you to select 8 key points from the calibration photo you just took. Before you do this, refer to [calibration-fixture.png](calibration/calibration-fixture.png) which shows the correct ordering for this process.

Run the script using the command ```python3 camera_calibration.py```. This opens the calibration photo you just took. Selecting the 8 key points in the calibration photo following the ordering shown in [calibration-fixture.png](calibration/calibration-fixture.png) by left clicking on each point (right click to cancel a selected point). Once all 8 points are selected, close the figure window to compute the camera matrix. This will update the [intrinsic parameters](calibration/param/intrinsic.txt). Note: keep the [distortion coefficients](calibration/param/distCoeffs.txt) to all 0s.


#### Step 3b) Camera calibration (Physical robot)
Similar to the previous step, you need to first run the [calib_pic.py](calibration/calib_pic.py) with the ```--ip``` and ```--port``` arguments, and then press ENTER to take a calibration photo (It will be saved as [calib_0.png](calibration/calib_0.png)). You have to drive the robot fairly close to the calibration rig to get a good view of the 8 dots. The photo should look something like this
![Real calibration photo](screenshots/RealCameraCalib.png?raw=true "Real calibration photo")

Once you have taken the [calib_0.png](calibration/calib_0.png) photo with the physical robot, proceed to the next step by running the command ```python3 camera_calibration.py``` and follow the remaining procedure in Step 3a.

### SLAM (week 4-5)
[operate.py](operate.py) makes use of the camera and wheels' [calibrated parameters](calibration/param) and the [SLAM](slam/) components to produce a map saved as "slam.txt" in the lab_output folder. This SLAM map contains a list of identified ARUCO makers, their locations and the covariances of the estimation. Note: remember to replace the [keyboard control section](operate.py#L194) with your codes from M1.

[SLAM](slam/ekf.py) computes the locations of the ARUCO markers using both [camera based estimation](slam/aruco_detector.py) and [motion model based estimation](slam/robot.py).

**Please complete [robot.py](slam/robot.py) by filling in the computation of the [derivatives](slam/robot.py#L79) and [covariance](slam/robot.py#L127) of the motion model. Please also complete [ekf.py](slam/ekf.py) by filling in the computation of the [predicted robot state](slam/ekf.py#L93) and the [updated robot state](slam/ekf.py#L117) to finish the extended Kalman filter function.** 

Once robot.py and ekf.py are completed, you can test the performance of your SLAM by running ```python3 operate.py```

![Example SLAM visualization in Gazebo](screenshots/GazeboSLAMvis.png?raw=true "Example SLAM visualization in Gazebo")

#### Evaluating the SLAM performance
An evaluation script [SLAM_eval.py](SLAM_eval.py) is provided for evaluating the map generated by SLAM against the true map (which can be saved by running ```rosrun penguinpi_gazebo scene_manager.py -s TRUEMAP.txt```). Note: the -s setting will generate a new map. If you want to use map1.txt (the arena from w1-2) for testing, please use ```rosrun penguinpi_gazebo scene_manager.py -l map1.txt```.

Run ```python3 SLAM_eval.py TRUEMAP.txt lab_output/slam.txt``` and you should see a printout of the evalution results:
 
![Example output of SLAM evaluation script](screenshots/SLAM_eval_output.png?raw=true "Example output of SLAM evaluation script")

## M2 Marking
### Evaluation scheme
To allow for the best performance of your SLAM module, you will be marked based on finding the 10 ARUCO markers, *the RMSE after alignment* between your estimations and the true locations of these markers during a live demonstration in both the simulation and the physical robot conducted in a **NEW MAP** in week 6. After the live demo, the map generated will be marked against the groundtruth map using [SLAM_eval.py](SLAM_eval.py). 5% of the total mark will be deducted for each marker the robot has collided into. Your M2 mark is computed as using the following:

**For simulation evaluation:**
simulation_score = (0.2 - Aligned_RMSE)/(0.2 - 0.025) x 80 + NumberOfFoundMarkers x 2

**For physical robot evaluation:**
robot_score = (0.15 - Aligned_RMSE)/(0.15 - 0.02) x 80 + NumberOfFoundMarkers x 2

**Total mark = 0.8 x simulation_score + 0.2 x robot_score - 0.05 x NumberOfCollidedMarkers**

*For the remote session, total mark = simulation score

**Note:** If your Aligned_RMSE value goes beyond the upper bound (0.2 for sim and 0.15 for robot), your score will be 0. Similarly, if the value goes below the lower bound, (0.025 for sim and 0.02 for robot), your will get full score. 

### Marking instructions
Please see [M2 marking instructions](M2_marking_instructions.md)

## FAQs: M2
- If you are using Mac to run the VM and are encountering performance issues, please follow the steps in [this link](https://www.reddit.com/r/virtualbox/comments/houi9k/how_to_fix_virtualbox_61_running_slow_on_mac/) 
- Refer to the comments in each of the [calibrateWheelRadius](calibration/wheel_calibration.py#L10) and [calibrateBaseline](calibration/wheel_calibration.py#L52) functions for that each of these values corresponds to on the physical robot
- Take a close look at the units of the expected output when formulating your calculations. Referring to these equations may be helpful:

![Useful equations for calculating baseline](screenshots/AngularVelocity.png?raw=true "Useful equations for calculating baseline")

- It is recommended that you keep the file structure for this lab material (and future weeks) unchanged to avoid path errors
- Remember to reach out via Slack if you encounter issues with your VM between lab sessions
- Beware of the sign error that can happen with calculating the difference between the measurement and estimate in the correction step
- The state vector x will be appended with the aruco-marker measurements. Take a note of the location of x that should be updated in the motion model.
- In the prediction step, we should update the mean belief by driving the robot.
